# micrograd

A small autograd engine inspired by [PyTorch Autograd Engine](https://pytorch.org/blog/overview-of-pytorch-autograd-engine/). Implements backpropagation over a dynamically built DAG and small neural networks library with a PyTorch-like API. DAG only operates over scalar values (so we chop up each neuron into all of its individual operations). This autograd engine is powerful enough to build deep neural networks implementing binary classification. 

This source code is commented to demonstrate deeper understanding of core concepts and for ease of future understanding. Further implementing and practicing backpropagation core concepts studied in [Neural Networks and Deep Learning by DeepLearning.AI and Andrew Ng](https://www.coursera.org/account/accomplishments/verify/ZJKF2ULGZVMS) 

This project is used for educational purposes and follows the course [Neural Networks: Zero to Hero by Andrej Karpathy](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=1) [Example Source Code](https://github.com/karpathy/micrograd)

## Installation
## Example usage
## License
MIT

